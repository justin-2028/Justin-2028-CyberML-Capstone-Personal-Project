---
title: "Cyber ML Capstone Project - Justin Oh"
author: "Justin Oh"
date: "11/22/2023"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

Our ever-increasing global connectivity and heavy reliance on cloud services has forced humanity to become more technologically dependent than ever before. Approximately 5.25 billion people have access to and use the internet on a daily basis. In fact, from the year 2000 to 2022, the usage of the internet has increased by 1,355%.

Such dependence highlights just how critical it has become for individuals, companies, and nations to boost their cyber security. Technology continues to advance at a rate nearly impossible to keep up with, but we must not trail behind in our attempts to understand these developments and apply the lessons learned. The cost of leniency is steep as cyber criminals are finding resounding success in their hacking attempts. To put in perspective, the average cost of a data breach was $4.24 million in 2021, with the average time to identify a breach being 212 days. 

While governments across the globe are allocating more resources to adequately combat cyber crimes, the vast majority of the world do not consider cyber security to be a significant issue, let alone a national security threat. In order to emphasize the importance for countries to increase investment in their cyber security programs, I have decided to pursue a personal project regarding the classification of benign and malicious URLs.

## Goal of the Project

By utilizing Logistic Regression, K-nearest neighbors (KNN) and Random Forest algorithms, the objective of this project is to identify which model most accurately classifies the URLs as malicious or benign. Specificity is the metric we'll try to improve as we work through the models. 


# Preparing the Dataset

First, we will load all the necessary libraries and the dataset. 

```{r,message=F,warning=F}
# Required packages will install, please allow several minutes to complete.

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(purrr)) install.packages("purrr", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(Hmisc)) install.packages("Hmisc", repos = "http://cran.us.r-project.org")
if(!require(forecast)) install.packages("forecast", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(class)) install.packages("class", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(psych)) install.packages("psych", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(NCmisc)) install.packages("NCmisc", repos = "http://cran.us.r-project.org")

# Packages for graphing and modeling
library(tidyverse)
library(purrr)
library(caret)
library(Hmisc)
library(forecast)
library(randomForest)
library(class)
library(data.table)
# Packages for reading the CSV file:
library(psych)
library(readr)
library(dplyr)
#Packages for testing what packages are used:
library(NCmisc)

```

Now, we will download the dataset from my GitHub repository and load it into RStudio: 

```{r,message=F,warning=F}
# The necessary data-set is able through my Github repository if the following code fails
download.file("https://raw.githubusercontent.com/justin-2028/Justin-2028-HarvardX-Capstone-Personal-Project/main/kaggleRCdataset.csv", "kaggleRCdataset.csv")
train = read.csv("kaggleRCdataset.csv", header = TRUE)
```

We'll move on to first stage of the analysis process, which is data cleaning.

# Data Description

URL: It is the anonymous identification of the URL analyzed in the study.

URL_LENGTH: It is the number of characters in the URL.

NUMBERSPECIALCHARACTERS: It is the number of special characters identified in the URL, such as, “/”, “%”, “#”, “&”, “. 
“, “=”.

CHARSET: It is a categorical value and represents the character encoding standard (also called character set).

SERVER: It is a categorical value and represents the operative system of the server that has been derived from the packet response.

CONTENT_LENGTH: It represents the content size of the HTTP header.

WHOIS_COUNTRY: It is a categorical variable, and its values are the countries we got from the server response (specifically, through the API of WHOIS).

WHOIS_STATEPRO: It is a categorical variable, and its values are the states we got from the server response (specifically, through the API of WHOIS).

WHOIS_REGDATE: WHOIS provides the server registration date, so, this variable has date values with the format DD/MM/YYY HH:MM.

WHOISUPDATEDDATE: Through the WHOIS, the last update date from the server is represented through this variable.

TCPCONVERSATIONEXCHANGE: This variable is the number of TCP packets exchanged between the server and the honeypot client.

DISTREMOTETCP_PORT: It is the number of the ports detected and unique to TCP.

REMOTE_IPS: This variable represents the total number of IPs connected to the honeypot.

APP_BYTES: This is the number of bytes transferred.

SOURCEAPPPACKETS: This is the number of packets sent from the honeypot to the server.

REMOTEAPPPACKETS: This is the number of packets received from the server.

APP_PACKETS: This is the total number of IP packets generated during the communication between the honeypot and the server.

DNSQUERYTIMES: This is the number of DNS packets generated during the communication between the honeypot and the server.

TYPE: This is a categorical variable, and its values represent the type of web page analyzed, specifically, 1 is for malicious websites and 0 is for benign websites.


# Data Cleaning:

Data cleaning is a vital stage of the analysis. It helps us understands the data and clean it in order to make it fit for our modelling purposes. Our data also has some issues that must be addressed beforehand. Thus, we will clean it to prepare it for modelling. 
There are some issues with the "WHOIS_COUNTRY" variable which we need to examine:

```{r}
unique(train$WHOIS_COUNTRY)
```

We can see that WHOIS_COUNTRY has different values for one country and that has to be corrected. For example: UK is shown as United Kingdom and GB. We need to correct that before moving forward.

```{r}
train$WHOIS_COUNTRY <- as.character(train$WHOIS_COUNTRY)
train[train$WHOIS_COUNTRY == 'United Kingdom','WHOIS_COUNTRY'] <- "UK"
train[train$WHOIS_COUNTRY == "[u'GB'; u'UK']",'WHOIS_COUNTRY'] <- "UK"
train[train$WHOIS_COUNTRY == "GB",'WHOIS_COUNTRY'] <- "UK"
train[train$WHOIS_COUNTRY == "us",'WHOIS_COUNTRY'] <- "US"
train[train$WHOIS_COUNTRY == 'ru','WHOIS_COUNTRY'] <- "RU"
```

Most countries do not seem to have malicious data. This disparity in data can lead to over-fitting in the modelling. We'll have to deal with this issue here by creating a single category for such countries called "Other".

```{r}
mc <- train[train$Type == 1,'WHOIS_COUNTRY']
others <- which(!(train$WHOIS_COUNTRY %in% mc))
train[others,'WHOIS_COUNTRY'] <- "Other"
train$WHOIS_COUNTRY <- as.factor(train$WHOIS_COUNTRY)

```


Now let's look at the values in CHARSET variable: 

```{r}
unique(train$CHARSET)
```


There is a similar problem in this variable as we saw in the WHOIS_COUNTRY variable: 

```{r}
train$CHARSET <- as.character(train$CHARSET)
train[train$CHARSET == 'iso-8859-1',"CHARSET"] <- "ISO-8859-1"
train[train$CHARSET == 'utf-8',"CHARSET"] <- "UTF-8"
train[train$CHARSET == 'windows-1251',"CHARSET"] <- "windows-12##"
train[train$CHARSET == 'windows-1252',"CHARSET"] <- "windows-12##"
train$CHARSET <- as.factor(train$CHARSET)
```

For the normalization of the SERVER variable, we will assign the values which do not have any malicious value in the data to the "Other" server value.

```{r}
train$SERVER <- as.character(train$SERVER)
mserver <- train[train$Type == 1,"SERVER"]
others <- which(!(train$SERVER %in% mserver))
train[others,'SERVER'] <- "Other"
table(train$SERVER == "Other")
train$SERVER <- as.factor(train$SERVER)
```


Having missing values in the data is a problem. There are many ways to deal with the missing values. The first method is to remove the missing values, but if the number of rows having missing values is high, it can result in data loss. Another method to deal with missing values is using imputations. This method helps when rows for missing values is high enough to not be removed, so you change the missing values to the mean of the variable. If the number of rows with missing values is more than 60% or 70% of total number of rows, then imputing the mean or median won't be helpful and it's better to remove that column from the analysis. We'll look at the number of NAs in our data:

```{r}
colSums(is.na(train))
```

We can see that there are 812 NAs in the content length variable. This is almost half of the total number of rows. We can remove this column as imputation won't be helpful in the modelling process and information from this variable isn't significant. There is 1 NA in DNS_QUERY_TIMES which can be solved by imputation:

```{r}
train$DNS_QUERY_TIMES=impute(train$DNS_QUERY_TIMES, 0)
train$CONTENT_LENGTH=impute(train$CONTENT_LENGTH, mean)
```


We'll now remove the variables which will not be useful in the modelling process:

```{r}
train$URL <- NULL     
train$WHOIS_REGDATE <- NULL

train$CONTENT_LENGTH <- NULL
```


We can change our response variable to the factor variable: 

```{r}
train$type<- as.factor(train$Type)
```

# Data Exploration:

The main purpose of exploring the data here is to get a better understanding of the dataset. First, we'll look at how many rows and columns we have in the dataset: 

```{r}
dim(train)
```

We have 1781 rows and 21 variables. 
Let's look at the first few and last few rows of the data set to see how the table looks like: 

```{r}
head(train, 10)
tail(train, 10)
```

Looking at the structure of the dataset is very important as it enables further understanding of the variables. It's important to make sure the variables have the correct classes. 

```{r}
str(train)
```

Now, let's look at the overall summary of the dataset to see the mean, median and variance for the numeric variables.

```{r}
describe(train)
```

It's important to look at the relationship of numeric variables to our response variable type. 
First, we'll look at the DNS_QUERY_TIMES variable against type variable: 

```{r}
ggplot(train, aes(x=as.factor(Type), y=DNS_QUERY_TIMES, fill=as.factor(Type))) + 
  geom_boxplot() +
  theme(legend.position="none")+
  ggtitle("Boxplot of Type by DNS_QUERY_TIMES")+
  xlab('Type')
  theme(plot.title = element_text(hjust = 0.5))
```

There seems to be clear difference between distribution of DNS_QUERY_TIMES for Malicious and Benign URLs. This suggests that DNS_QUERY_TIMES can be a good predictor.

Now we'll look at the relationship for APP_PACKETS variable:

```{r}
train%>%ggplot(aes(x=as.factor(Type), y=APP_PACKETS, fill=as.factor(Type))) + 
  geom_boxplot() +
  theme(legend.position="none")+
  ggtitle("Boxplot of Type by App Packets")+
  xlab('Type')+ylab('App Packets')+
  theme(plot.title = element_text(hjust = 0.5))
```

There are a lot of outliers in this variable and there seems to be no difference between malicious and benign URLs for the APP_PACKETS variable. Our model will explain if this variable has potential to be a good predictor.

Let's look at the TCP_CONVERSATION_EXCHANGE variable: 

```{r}
train%>%ggplot(aes(x=as.factor(Type), y=TCP_CONVERSATION_EXCHANGE, fill=as.factor(Type))) + 
  geom_boxplot() +
  theme(legend.position="none")+
  ggtitle("Boxplot of Type by TCP Conversation Exchange")+
  xlab('Type')+ylab('TCP Conversation Exchange')+
  theme(plot.title = element_text(hjust = 0.5))
```

The distribution is similar to the App Packets and there are a lot of outliers in the dataset. 
Now let's look at the remote IPS variable: 

```{r}
train%>%ggplot(aes(x=as.factor(Type), y=REMOTE_IPS, fill=as.factor(Type))) + 
  geom_boxplot() +
  theme(legend.position="none")+
  ggtitle("Boxplot of Type by Remote IPS")+
  xlab('Type')+ylab('Remote IPS')+
  theme(plot.title = element_text(hjust = 0.5))
```

Remote IPS has a similar median for Malicious and Benign URLs. However, the interquartile range of Malicious links is less than that of Benign URLs. 
Now, we'll look at the length of the URLs to see the distribution of it for our response variable.

```{r}
train%>%ggplot(aes(x=as.factor(Type), y=URL_LENGTH, fill=as.factor(Type))) + 
  geom_boxplot() +
  theme(legend.position="none")+
  ggtitle("Boxplot of Type by URL Length")+
  xlab('Type')+ylab('URL Length')+
  theme(plot.title = element_text(hjust = 0.5))
```

The shape of the distribution for Malicious and Benign URLs of URL length is quite different, so this can be a good identifier for the type of URL. 
Finally, we'll plot the distribution of all the numeric variables and see if there's a pattern: 

```{r}
train %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()
```

It can be observed that most of the variables are skewed to the right and some variables have a spread out distribution. 
The variables with a spread out distribution can be helpful in prediction.

# Creating the Test and Train Dataset: 

First of all, we'll do the min-max normalization on the data and then create the training and test dataset. Our training dataset will be 80% of the observations. 

```{r,message=F,warning=F}
minMax <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) }

train.subset <- train[c('URL_LENGTH','NUMBER_SPECIAL_CHARACTERS','TCP_CONVERSATION_EXCHANGE','DIST_REMOTE_TCP_PORT','REMOTE_IPS','APP_BYTES','SOURCE_APP_PACKETS','REMOTE_APP_PACKETS', 'Type')]

train.subset.n <- as.data.frame(lapply(train.subset, minMax))
head(train.subset.n)

set.seed(123)
test_index <- sample(1:nrow(train.subset.n),size=nrow(train.subset.n)*0.2,replace = FALSE) #random selection of 20% data.

test.data <- train.subset[test_index,] # 20% will be test data
train.data <- train.subset[-test_index,] # remaining 80% will be training data

#Creating separate data frame for the 'Type' feature which is our target.
test.data_labels <-train.subset[test_index,9]
train.data_labels <- train.subset[-test_index,9]
```

# Modelling and Testing: 

We'll fit the Logistic Regression, KNN and Random forest models on the data and determine the performance of the models. The performance metrics that are important are Accuracy, Specificity and Sensitivity. Specificity tells us that how many Malicious URLs were correctly predicted by the model. That's why this is the most important metric for us as mentioned in the Introduction section.

## Logistic Regression:


Now, we have our data ready for modelling. First, we'll use the logistic regression model. Logistic regression is quite a simple model which finds the logit function of the probability of the response variable using the equation used in linear modelling: 
$logit(p) = \beta_0 + \beta_i \times X_i + \epsilon_i$
We'll fit the logistic regression model on our data and predict it through our test data to check the accuracy and specificity of the results. Specificity is an important metric for us as it tells us how many of malicious URLs were correctly predicted by the algorithm. 
Let's fit the model and check the summary of the results: 

```{r}

glm_model <- glm(Type~.,data = train.data)

#Looking at the summary of the model
summary(glm_model)
```

We can see in the results of the model that all of the models are significant as their p-value is less than 0.05 level of significance. The AIC of the model is 592, which looks good for our purposes. Let's test the performance of the model on the test data: 

```{r}
predictions<- as.factor(ifelse(predict(glm_model, newdata = test.data,type = 'response')>0.5,1,0))

confusionMatrix(predictions,as.factor(test.data_labels))
```

Our logistic regression model gives us an accuracy of 88.2%, which isn't bad. However, we can see that the model is over-fitted on the data because the specificity of the model is very low (2.4%). The no information rate of the model is also very high. The results of the logistic regression model are not what we are looking for, so we'll test other models to achieve a higher accuracy.

## KNN: 

The KNN model checks all the data and classifies based on the similarity in the data. It is a non-parametric, supervised learning model and similar data is classified based on the nearest neighbor approach. It categorizes data based on similarity and classifies new cases based on their similarity with available categories. 
One important parameter in this model is the value of K. It represents the number of neighbors for assigning categories. We'll determine the optimal number of K through iterations. 
Let's fit KNN onto our data. We'll run a loop for KNN from 1 to 100 to test value of K and check accuracy at each point. The value of K which gives us highest accuracy will be selected. 

```{r}
i=1                          # declaration to initiate for loop
k.optm=1                     # declaration to initiate for loop
for (i in 1:100){ 
  knn.mod <-  knn(train=train.data, test=test.data, cl=train.data_labels, k=i)
  k.optm[i] <- 100 * sum(test.data_labels == knn.mod)/NROW(test.data_labels)
  k=i  
  cat(k,'=',k.optm[i],'\n')       # to print % accuracy 
}
```

The series shows us the accuracy against value of K. 

```{r}
plot(k.optm, type="b", xlab="K- Value",ylab="Accuracy level")
```

As we can see from the plot and series, the optimal value of K is 17 as it yields the highest accuracy. 
Now we'll fit the KNN for the K value of 17: 

```{r}
knn.17 <- knn(train=train.data, test=test.data, cl=train.data_labels, k=17)
confusionMatrix(knn.17, as.factor(test.data_labels))

```

The accuracy of the model is 91.8 which is better than our previous model (logistic regression). The specificity of this KNN model is 39%, which is far better than logistic regression but it's still not what we are looking for. Now, we'll test our final model, Random Forest.

## Random Forest: 

The Random Forest model uses multiple decision trees to determine classifications. Random Forest classifies based on the general consensus provided from the decision trees involved.
Let's fit the model and check the variable importance. 

```{r}
train.data$Type <- as.factor(train.data$Type)
modelrf <- randomForest(Type ~ ., data=train.data,ntree=1000)
(varimp.modelrf <- varImp(modelrf))
```

We can see that URL_LENGTH and NUMBER_SPECIAL_CHARACTERS are the most important variables in this process. Let's test the output of the model on test data: 

```{r}
test.data$rf.pred <- predict(modelrf, newdata = test.data)
head(test.data[c("Type","rf.pred")], n=10)
confusionMatrix(as.factor(test.data$Type),as.factor(test.data$rf.pred))
```

We can see that output of the Random Forest model is great on the test set. The overall accuracy of the model is 97% and specificity of the model is 94%. This result is great for our purposes and we'll use this model over the others. 

# Results

Over the course of this report, Logistic Regression, K-nearest neighbors (KNN) and Random Forest algorithms were utilized to find the model that best predicted whether a URL was malicious or benign. 

We achieved a notable increase in accuracy from 88.2% to 97.47%, and an incredible jump in specificity from 2.4% to approximately 94%, with Logistic Regression being the former and Random Forests the latter. 

Having accomplished both the highest accuracy and specificity amongst the three models, it is evident that the Random Forest algorithm is the best predictor of malicious or benign URLs. 

# Conclusion

Through this analysis, we were able to confirm that machine learning approaches regarding the prediction of Malicious and Benign URLs can be extremely helpful as they can decrease the risk of cyber crime and theft when applied successfully. In our analysis, the Random Forest model was able to secure a 94% Malicious URL prediction rate, which serves as a testament to the usefulness of such algorithms. Other advanced approaches (ex. neural network) can be more accurate in prediction. We saw that length of the URL and number of special characters in a URL are the most important when it comes to classification.

One important future implication is that a model trained through more detailed observations and with more advanced machine learning techniques can be deployed in real world situation. When considering the high prediction rates demonstrated by the models as a whole, their helpfulness becomes apparent, particularly in the cybersecurity sector.

While there is clear room for improvement, the algorithms used in this analysis found immense success in differentiating between Malicious and Benign URLs. Furthermore, we were able to determine that the Random Forest model produced the best results, making this venture an overall success.

## Limitations

My Cyber ML Capstone Project encountered several limitations that I hope to address in the future, perhaps through the assistance of my peer reviewers and the feedback of the staff.

1) Software/Hardware Limitations: I have attempted several advanced machine learning models that could have outbested the Random Forest models, such as a potential incorporation of the cforest package, but the lackluster processing power and RAM of my laptop has currently made it difficult to do so.
2) Exploratory Limitations: A higher understanding of the Malicious and Benign URLs dataset would have allowed me to fine-tune the models further. I had much difficulty with the data wrangling portion of the project, so I hope to improve that in future endeavors.
3) Dataset Limitations: The Malicious and Benign URLs dataset contains only 1781 unique URLs. The small sample size undoubtedly limited the performance of our models. However, other datasets on Kaggle either faced the same predicament or lacked the variables I was looking for. I will continue to search for a higher-quality dataset so I can revisit and improve this project.

## Future Works

As summarized above, my attempts at creating better models has been hampered by the limitations of my device and my understanding of the Malicious and Benign URLs dataset. However, I intend on trying the following in the near future.

1) Applying gradient boosting (through XGBoost) for regression purposes. As mentioned before, neural networks could be incorporated as well.
2) Redoing the project with a dataset with a much larger sample size. Could consider either broadening or narrowing the subject matter if necessary. 

## Acknowledgements

I would like to thank Professor Rafael Irizarry, as well as the countless educators, moderators, and supervisors that have contributed to the HarvardX Professional Data Science Certificate Program. Their collective efforts have provided one of the best remote learning experiences I have had to date, and I am extremely grateful to have studied the R programming language under such accredited curriculum. I would like to thank Christian Urcuquii for generously providing the Malicious and Benign URLs dataset on Kaggle for public use. I also want to share my gratitude and heartfelt thanks to my fellow peers, whose discussions on the interactive forums have given me invaluable insight throughout the journey. Lastly, I would like to acknowledge my parents, who have never stopped supporting my dreams.